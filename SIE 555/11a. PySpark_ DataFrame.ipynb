{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 11a. PySpark: DataFrame.ipynb","provenance":[{"file_id":"13cEs4JoxUnfrJh7FqSU-zd8c97aMWi9U","timestamp":1649686770740}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"p0-YhEpP_Ds-"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"Zsj5WYpR9QId"},"source":["Let's set up Spark on your Colab environment.  Run the cell below!"]},{"cell_type":"code","metadata":{"id":"k-qHai2252mI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647282291569,"user_tz":240,"elapsed":79310,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"2ce3197b-d3f1-481e-d5da-9b554b757d7e"},"source":["!pip install pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 26 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.3\n","  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 42.3 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=d257824544850e7dbdc352f3e6cef724b1c8393b378c2f17c2eb065e06caf54e\n","  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n","The following additional packages will be installed:\n","  openjdk-8-jre-headless\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n","  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n","  fonts-wqy-zenhei fonts-indic\n","The following NEW packages will be installed:\n","  openjdk-8-jdk-headless openjdk-8-jre-headless\n","0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n","Need to get 36.5 MB of archives.\n","After this operation, 143 MB of additional disk space will be used.\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","(Reading database ... 155335 files and directories currently installed.)\n","Preparing to unpack .../openjdk-8-jre-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../openjdk-8-jdk-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"]}]},{"metadata":{"id":"GMHclPuRDUuM"},"cell_type":"markdown","source":["# Quickstart: DataFrame\n","\n","This is a short introduction and quickstart for the PySpark DataFrame API. PySpark DataFrames are lazily evaluated. They are implemented on top of [RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html#overview)s. When Spark [transforms](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) data, it does not immediately compute the transformation but plans how to compute later. When [actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions) such as `collect()` are explicitly called, the computation starts.\n","This notebook shows the basic usages of the DataFrame, geared mainly for new users. You can run the latest version of these examples by yourself in 'Live Notebook: DataFrame' at [the quickstart page](https://spark.apache.org/docs/latest/api/python/getting_started/index.html).\n","\n","There is also other useful information in Apache Spark documentation site, see the latest version of [Spark SQL and DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html), [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html), [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html), [Spark Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html) and [Machine Learning Library (MLlib) Guide](https://spark.apache.org/docs/latest/ml-guide.html).\n","\n","PySpark applications start with initializing `SparkSession` which is the entry point of PySpark as below. In case of running it in PySpark shell via <code>pyspark</code> executable, the shell automatically creates the session in the variable <code>spark</code> for users."]},{"metadata":{"trusted":false,"id":"K5ViKGCZDUuT"},"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"DjWtV3BiDUuW"},"cell_type":"markdown","source":["## DataFrame Creation\n","\n","A PySpark DataFrame can be created via `pyspark.sql.SparkSession.createDataFrame` typically by passing a list of lists, tuples, dictionaries and `pyspark.sql.Row`s, a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and an RDD consisting of such a list.\n","`pyspark.sql.SparkSession.createDataFrame` takes the `schema` argument to specify the schema of the DataFrame. When it is omitted, PySpark infers the corresponding schema by taking a sample from the data.\n","\n","Firstly, you can create a PySpark DataFrame from a list of rows"]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"cboqGI1eDUuX","executionInfo":{"status":"ok","timestamp":1647282308775,"user_tz":240,"elapsed":9627,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"97837c84-0c6b-4ef9-983d-cfe172b49662"},"cell_type":"code","source":["from datetime import datetime, date\n","import pandas as pd\n","from pyspark.sql import Row\n","\n","df = spark.createDataFrame([\n","    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n","    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n","    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n","])\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]},"metadata":{},"execution_count":3}]},{"metadata":{"id":"KyeM3NM0DUua"},"cell_type":"markdown","source":["Create a PySpark DataFrame with an explicit schema."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"BGj_bf95DUub","executionInfo":{"status":"ok","timestamp":1647282308776,"user_tz":240,"elapsed":7,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"ec7731d4-9367-499e-c6e0-9826c3dae56b"},"cell_type":"code","source":["df = spark.createDataFrame([\n","    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n","    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n","    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n","], schema='a long, b double, c string, d date, e timestamp')\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]},"metadata":{},"execution_count":4}]},{"metadata":{"id":"6ZsGnC6pDUub"},"cell_type":"markdown","source":["Create a PySpark DataFrame from a pandas DataFrame"]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"ggwHw9PeDUuc","executionInfo":{"status":"ok","timestamp":1647282309281,"user_tz":240,"elapsed":509,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"4f71c75f-55a5-465f-b1d3-614bdb67887d"},"cell_type":"code","source":["pandas_df = pd.DataFrame({\n","    'a': [1, 2, 3],\n","    'b': [2., 3., 4.],\n","    'c': ['string1', 'string2', 'string3'],\n","    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n","    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n","})\n","df = spark.createDataFrame(pandas_df)\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]},"metadata":{},"execution_count":5}]},{"metadata":{"id":"TEbLVAQiDUud"},"cell_type":"markdown","source":["Create a PySpark DataFrame from an RDD consisting of a list of tuples."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"cVNfS9PzDUue","executionInfo":{"status":"ok","timestamp":1647282313099,"user_tz":240,"elapsed":3824,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"d99088a8-8012-4979-90d1-50c4471ab035"},"cell_type":"code","source":["rdd = spark.sparkContext.parallelize([\n","    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n","    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n","    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n","])\n","df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]},"metadata":{},"execution_count":6}]},{"metadata":{"id":"oXMweYtxDUuf"},"cell_type":"markdown","source":["The DataFrames created above all have the same results and schema."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"j8Lr1G0PDUug","executionInfo":{"status":"ok","timestamp":1647282318580,"user_tz":240,"elapsed":5494,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"867d1ab7-1c65-41ca-cf3a-eb80058d980f"},"cell_type":"code","source":["# All DataFrames above result same.\n","df.show()\n","df.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+\n","|  a|  b|      c|         d|                  e|\n","+---+---+-------+----------+-------------------+\n","|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n","|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n","|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n","+---+---+-------+----------+-------------------+\n","\n","root\n"," |-- a: long (nullable = true)\n"," |-- b: double (nullable = true)\n"," |-- c: string (nullable = true)\n"," |-- d: date (nullable = true)\n"," |-- e: timestamp (nullable = true)\n","\n"]}]},{"metadata":{"id":"1z5ZORmrDUug"},"cell_type":"markdown","source":["## Viewing Data\n","\n","The top rows of a DataFrame can be displayed using `DataFrame.show()`."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"sNXvs5D9DUuh","executionInfo":{"status":"ok","timestamp":1647282319441,"user_tz":240,"elapsed":874,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"a4141c4c-83ad-433c-9442-1dce9d08d9b9"},"cell_type":"code","source":["df.show(1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+\n","|  a|  b|      c|         d|                  e|\n","+---+---+-------+----------+-------------------+\n","|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n","+---+---+-------+----------+-------------------+\n","only showing top 1 row\n","\n"]}]},{"metadata":{"id":"OMTV2wq7DUuh"},"cell_type":"markdown","source":["Alternatively, you can enable `spark.sql.repl.eagerEval.enabled` configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. The number of rows to show can be controlled via `spark.sql.repl.eagerEval.maxNumRows` configuration."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/","height":114},"id":"5K1NGzO1DUui","executionInfo":{"status":"ok","timestamp":1647282320126,"user_tz":240,"elapsed":687,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"95d26b53-011f-4fa6-b00d-7e1db1c9a863"},"cell_type":"code","source":["spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"],"text/html":["<table border='1'>\n","<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n","<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n","<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n","<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n","</table>\n"]},"metadata":{},"execution_count":9}]},{"metadata":{"id":"fe26QL0nDUui"},"cell_type":"markdown","source":["The rows can also be shown vertically. This is useful when rows are too long to show horizontally."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"qMggvohPDUui","executionInfo":{"status":"ok","timestamp":1647282320493,"user_tz":240,"elapsed":374,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"26c628a4-116d-4466-e0bf-82ead593ef69"},"cell_type":"code","source":["df.show(1, vertical=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-RECORD 0------------------\n"," a   | 1                   \n"," b   | 2.0                 \n"," c   | string1             \n"," d   | 2000-01-01          \n"," e   | 2000-01-01 12:00:00 \n","only showing top 1 row\n","\n"]}]},{"metadata":{"id":"jg1DkC-TDUuj"},"cell_type":"markdown","source":["You can see the DataFrame's schema and column names as follows:"]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"sryFfAYKDUuj","executionInfo":{"status":"ok","timestamp":1647282668602,"user_tz":240,"elapsed":437,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"4ba1523f-06e6-44c6-de54-4da29073b9df"},"cell_type":"code","source":["df.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['color', 'fruit', 'v1', 'v2']"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["df.dtypes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yqWtoLpsoxl","executionInfo":{"status":"ok","timestamp":1647282678505,"user_tz":240,"elapsed":316,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"aafe050d-b1ab-4fc8-f946-2afc7c7fbbfd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('color', 'string'), ('fruit', 'string'), ('v1', 'bigint'), ('v2', 'bigint')]"]},"metadata":{},"execution_count":36}]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"vVh5RsuQDUuk","executionInfo":{"status":"ok","timestamp":1647282320494,"user_tz":240,"elapsed":8,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"9f679f60-64aa-46c4-b521-3346cc2f2d71"},"cell_type":"code","source":["df.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- a: long (nullable = true)\n"," |-- b: double (nullable = true)\n"," |-- c: string (nullable = true)\n"," |-- d: date (nullable = true)\n"," |-- e: timestamp (nullable = true)\n","\n"]}]},{"metadata":{"id":"vJ0hJmZRDUul"},"cell_type":"markdown","source":["Show the summary of the DataFrame"]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"cnIv_qT_DUul","executionInfo":{"status":"ok","timestamp":1647282325075,"user_tz":240,"elapsed":4585,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"dc9c989e-b369-4293-ea57-185b393210cf"},"cell_type":"code","source":["df.select(\"a\", \"b\", \"c\").describe().show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+---+---+-------+\n","|summary|  a|  b|      c|\n","+-------+---+---+-------+\n","|  count|  3|  3|      3|\n","|   mean|2.0|3.0|   null|\n","| stddev|1.0|1.0|   null|\n","|    min|  1|2.0|string1|\n","|    max|  3|4.0|string3|\n","+-------+---+---+-------+\n","\n"]}]},{"metadata":{"id":"MtCDGnMkDUul"},"cell_type":"markdown","source":["`DataFrame.collect()` collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"pPNzJEbcDUun","executionInfo":{"status":"ok","timestamp":1647282325424,"user_tz":240,"elapsed":367,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"70e514ed-e12a-4809-8fce-6f7ac9ece2e9"},"cell_type":"code","source":["df.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n"," Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n"," Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"]},"metadata":{},"execution_count":14}]},{"metadata":{"id":"GDmdltwkDUun"},"cell_type":"markdown","source":["In order to avoid throwing an out-of-memory exception, use `DataFrame.take()` or `DataFrame.tail()`."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"qAOAQ7YFDUun","executionInfo":{"status":"ok","timestamp":1647282325425,"user_tz":240,"elapsed":8,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"c6cb9397-3058-4910-cd4c-112e9244d9e7"},"cell_type":"code","source":["df.take(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]"]},"metadata":{},"execution_count":15}]},{"metadata":{"id":"wScrrKAgDUuo"},"cell_type":"markdown","source":["PySpark DataFrame also provides the conversion back to a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to leverage pandas API. Note that `toPandas` also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"sekwOr4wDUuo","executionInfo":{"status":"ok","timestamp":1647282325914,"user_tz":240,"elapsed":495,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"af9ca34d-b5db-469b-9834-ffad089b6c07"},"cell_type":"code","source":["df.toPandas()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   a    b        c           d                   e\n","0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n","1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n","2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00"],"text/html":["\n","  <div id=\"df-7de32827-43c2-430a-a592-4265a32062a1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>a</th>\n","      <th>b</th>\n","      <th>c</th>\n","      <th>d</th>\n","      <th>e</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>string1</td>\n","      <td>2000-01-01</td>\n","      <td>2000-01-01 12:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>3.0</td>\n","      <td>string2</td>\n","      <td>2000-02-01</td>\n","      <td>2000-01-02 12:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>4.0</td>\n","      <td>string3</td>\n","      <td>2000-03-01</td>\n","      <td>2000-01-03 12:00:00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7de32827-43c2-430a-a592-4265a32062a1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7de32827-43c2-430a-a592-4265a32062a1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7de32827-43c2-430a-a592-4265a32062a1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}]},{"metadata":{"id":"-84v4TkBDUup"},"cell_type":"markdown","source":["## Selecting and Accessing Data\n","\n","PySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a `Column` instance."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"vfJPQWzdDUup","executionInfo":{"status":"ok","timestamp":1647282326536,"user_tz":240,"elapsed":629,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"af601ce5-557e-4119-df9d-26707fbca1d5"},"cell_type":"code","source":["df.a"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Column<'a'>"]},"metadata":{},"execution_count":17}]},{"metadata":{"id":"n4hPEt32DUup"},"cell_type":"markdown","source":["In fact, most of column-wise operations return `Column`s."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"wygWF1VSDUup","executionInfo":{"status":"ok","timestamp":1647282326537,"user_tz":240,"elapsed":6,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"647284c0-9dfb-41ce-cdc9-8000e0e3f117"},"cell_type":"code","source":["from pyspark.sql import Column\n","from pyspark.sql.functions import upper\n","\n","type(df.c) == type(upper(df.c)) == type(df.c.isNull())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}]},{"metadata":{"id":"Xlnau6R2DUuq"},"cell_type":"markdown","source":["These `Column`s can be used to select the columns from a DataFrame. For example, `DataFrame.select()` takes the `Column` instances that returns another DataFrame."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"rwIO9cSDDUuq","executionInfo":{"status":"ok","timestamp":1647282326935,"user_tz":240,"elapsed":402,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"c1568809-4b06-4dd1-fd9e-c6cc98e4307d"},"cell_type":"code","source":["df.select(df.c).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+\n","|      c|\n","+-------+\n","|string1|\n","|string2|\n","|string3|\n","+-------+\n","\n"]}]},{"metadata":{"id":"WadcDhQHDUuq"},"cell_type":"markdown","source":["Assign new `Column` instance."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"3y_qVdCoDUur","executionInfo":{"status":"ok","timestamp":1647282327375,"user_tz":240,"elapsed":443,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"b46a6a1d-e239-4309-bb55-91ca67718949"},"cell_type":"code","source":["df.withColumn('upper_c', upper(df.c)).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+-------+\n","|  a|  b|      c|         d|                  e|upper_c|\n","+---+---+-------+----------+-------------------+-------+\n","|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n","|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n","|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n","+---+---+-------+----------+-------------------+-------+\n","\n"]}]},{"metadata":{"id":"jgriOKqBDUur"},"cell_type":"markdown","source":["To select a subset of rows, use `DataFrame.filter()`."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"TYTLMEByDUur","executionInfo":{"status":"ok","timestamp":1647282328053,"user_tz":240,"elapsed":684,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"93abf59b-67f5-4420-e347-1b4c52fb2774"},"cell_type":"code","source":["df.filter(df.a == 1).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+\n","|  a|  b|      c|         d|                  e|\n","+---+---+-------+----------+-------------------+\n","|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n","+---+---+-------+----------+-------------------+\n","\n"]}]},{"metadata":{"id":"P2JnBcTfDUur"},"cell_type":"markdown","source":["## Applying a Function\n","\n","PySpark supports various UDFs and APIs to allow users to execute Python native functions. See also the latest [Pandas UDFs](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs) and [Pandas Function APIs](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#pandas-function-apis). For instance, the example below allows users to directly use the APIs in [a pandas Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) within Python native function."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"OhMEUVrwDUus","executionInfo":{"status":"ok","timestamp":1647282332027,"user_tz":240,"elapsed":3977,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"cc0b54c4-d244-4ab4-9e0c-243a2528f819"},"cell_type":"code","source":["import pandas\n","from pyspark.sql.functions import pandas_udf\n","\n","@pandas_udf('long')\n","def pandas_plus_one(series: pd.Series) -> pd.Series:\n","    # Simply plus one by using pandas Series.\n","    return series + 1\n","\n","df.select(pandas_plus_one(df.a)).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+\n","|pandas_plus_one(a)|\n","+------------------+\n","|                 2|\n","|                 3|\n","|                 4|\n","+------------------+\n","\n"]}]},{"metadata":{"id":"qT5kiQHgDUus"},"cell_type":"markdown","source":["Another example is `DataFrame.mapInPandas` which allows users directly use the APIs in a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) without any restrictions such as the result length."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"mUtkfNTiDUus","executionInfo":{"status":"ok","timestamp":1647282332698,"user_tz":240,"elapsed":674,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"3bc49b39-9517-4120-8809-677a1dff27d6"},"cell_type":"code","source":["def pandas_filter_func(iterator):\n","    for pandas_df in iterator:\n","        yield pandas_df[pandas_df.a == 1]\n","\n","df.mapInPandas(pandas_filter_func, schema=df.schema).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+\n","|  a|  b|      c|         d|                  e|\n","+---+---+-------+----------+-------------------+\n","|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n","+---+---+-------+----------+-------------------+\n","\n"]}]},{"metadata":{"id":"IEshmJSUDUus"},"cell_type":"markdown","source":["## Grouping Data\n","\n","PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy.\n","It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"ctXjADwwDUut","executionInfo":{"status":"ok","timestamp":1647282333386,"user_tz":240,"elapsed":693,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"63f2b552-ba1c-4fd5-8551-eda62141666d"},"cell_type":"code","source":["df = spark.createDataFrame([\n","    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n","    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n","    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n","df.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+------+---+---+\n","|color| fruit| v1| v2|\n","+-----+------+---+---+\n","|  red|banana|  1| 10|\n","| blue|banana|  2| 20|\n","|  red|carrot|  3| 30|\n","| blue| grape|  4| 40|\n","|  red|carrot|  5| 50|\n","|black|carrot|  6| 60|\n","|  red|banana|  7| 70|\n","|  red| grape|  8| 80|\n","+-----+------+---+---+\n","\n"]}]},{"metadata":{"id":"OYMXWvmTDUut"},"cell_type":"markdown","source":["Grouping and then applying the `avg()` function to the resulting groups."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"iaNWOiXBDUut","executionInfo":{"status":"ok","timestamp":1647282335332,"user_tz":240,"elapsed":1949,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"3ba81aa7-6709-46c1-f688-50fd78ff8ac7"},"cell_type":"code","source":["df.groupby('color').avg().show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+-------+-------+\n","|color|avg(v1)|avg(v2)|\n","+-----+-------+-------+\n","|  red|    4.8|   48.0|\n","| blue|    3.0|   30.0|\n","|black|    6.0|   60.0|\n","+-----+-------+-------+\n","\n"]}]},{"metadata":{"id":"OUQK-wfODUut"},"cell_type":"markdown","source":["You can also apply a Python native function against each group by using pandas API."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"SNYc0l9PDUut","executionInfo":{"status":"ok","timestamp":1647282336775,"user_tz":240,"elapsed":1447,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"8a6f09af-7ea9-4e16-92cb-c75d577486cc"},"cell_type":"code","source":["def plus_mean(pandas_df):\n","    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n","\n","df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+------+---+---+\n","|color| fruit| v1| v2|\n","+-----+------+---+---+\n","|black|carrot|  0| 60|\n","| blue|banana| -1| 20|\n","| blue| grape|  1| 40|\n","|  red|banana| -3| 10|\n","|  red|carrot| -1| 30|\n","|  red|carrot|  0| 50|\n","|  red|banana|  2| 70|\n","|  red| grape|  3| 80|\n","+-----+------+---+---+\n","\n"]}]},{"metadata":{"id":"eulqIa1kDUuu"},"cell_type":"markdown","source":["Co-grouping and applying a function."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"9MBD4ZN8DUuv","executionInfo":{"status":"ok","timestamp":1647282337929,"user_tz":240,"elapsed":1174,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"a2fcd520-224d-4320-9820-bbb8d262b14a"},"cell_type":"code","source":["df1 = spark.createDataFrame(\n","    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n","    ('time', 'id', 'v1'))\n","\n","df2 = spark.createDataFrame(\n","    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n","    ('time', 'id', 'v2'))\n","\n","def asof_join(l, r):\n","    return pd.merge_asof(l, r, on='time', by='id')\n","\n","df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n","    asof_join, schema='time int, id int, v1 double, v2 string').show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+---+---+---+\n","|    time| id| v1| v2|\n","+--------+---+---+---+\n","|20000101|  1|1.0|  x|\n","|20000102|  1|3.0|  x|\n","|20000101|  2|2.0|  y|\n","|20000102|  2|4.0|  y|\n","+--------+---+---+---+\n","\n"]}]},{"metadata":{"id":"IlCwAtF5DUuv"},"cell_type":"markdown","source":["## Getting Data in/out\n","\n","CSV is straightforward and easy to use. Parquet and ORC are efficient and compact file formats to read and write faster.\n","\n","There are many other data sources available in PySpark such as JDBC, text, binaryFile, Avro, etc. See also the latest [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) in Apache Spark documentation."]},{"metadata":{"id":"HgsaqPXIDUuv"},"cell_type":"markdown","source":["### CSV"]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"mFDWHAjdDUuw","executionInfo":{"status":"ok","timestamp":1647282341893,"user_tz":240,"elapsed":3966,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"dce6b877-085b-4b9c-9ed3-b5a93061a422"},"cell_type":"code","source":["df.write.csv('foo.csv', header=True)\n","spark.read.csv('foo.csv', header=True).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+------+---+---+\n","|color| fruit| v1| v2|\n","+-----+------+---+---+\n","|  red|banana|  1| 10|\n","| blue|banana|  2| 20|\n","|  red|carrot|  3| 30|\n","| blue| grape|  4| 40|\n","|  red|carrot|  5| 50|\n","|black|carrot|  6| 60|\n","|  red|banana|  7| 70|\n","|  red| grape|  8| 80|\n","+-----+------+---+---+\n","\n"]}]},{"metadata":{"id":"JqUH22_uDUuw"},"cell_type":"markdown","source":["### Parquet"]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"KlP4F1-rDUuw","executionInfo":{"status":"ok","timestamp":1647282345679,"user_tz":240,"elapsed":3800,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"05f049ba-8572-4849-e20e-9d21539a3fe4"},"cell_type":"code","source":["df.write.parquet('bar.parquet')\n","spark.read.parquet('bar.parquet').show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+------+---+---+\n","|color| fruit| v1| v2|\n","+-----+------+---+---+\n","|  red|carrot|  5| 50|\n","|black|carrot|  6| 60|\n","|  red|banana|  7| 70|\n","|  red| grape|  8| 80|\n","|  red|banana|  1| 10|\n","| blue|banana|  2| 20|\n","|  red|carrot|  3| 30|\n","| blue| grape|  4| 40|\n","+-----+------+---+---+\n","\n"]}]},{"metadata":{"id":"PgHpKj0bDUuw"},"cell_type":"markdown","source":["### ORC"]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"ko8KznuNDUux","executionInfo":{"status":"ok","timestamp":1647282346969,"user_tz":240,"elapsed":1308,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"115a054d-72d0-4054-9d35-31183e079adf"},"cell_type":"code","source":["df.write.orc('zoo.orc')\n","spark.read.orc('zoo.orc').show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+------+---+---+\n","|color| fruit| v1| v2|\n","+-----+------+---+---+\n","|  red|carrot|  5| 50|\n","|black|carrot|  6| 60|\n","|  red|banana|  7| 70|\n","|  red| grape|  8| 80|\n","|  red|banana|  1| 10|\n","| blue|banana|  2| 20|\n","|  red|carrot|  3| 30|\n","| blue| grape|  4| 40|\n","+-----+------+---+---+\n","\n"]}]},{"metadata":{"id":"3DvO6vocDUux"},"cell_type":"markdown","source":["## Working with SQL\n","\n","DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below:"]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"lnuWLiFuDUux","executionInfo":{"status":"ok","timestamp":1647282347717,"user_tz":240,"elapsed":751,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"04836355-09de-4640-de56-442b53d8f5a7"},"cell_type":"code","source":["df.createOrReplaceTempView(\"tableA\")\n","spark.sql(\"SELECT count(*) from tableA\").show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+\n","|count(1)|\n","+--------+\n","|       8|\n","+--------+\n","\n"]}]},{"metadata":{"id":"Y7aZdC3TDUux"},"cell_type":"markdown","source":["In addition, UDFs can be registered and invoked in SQL out of the box:"]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"fM0iZYM_DUuy","executionInfo":{"status":"ok","timestamp":1647282348235,"user_tz":240,"elapsed":530,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"fc5a5864-44fd-4be5-d377-19f339eac2f1"},"cell_type":"code","source":["@pandas_udf(\"integer\")\n","def add_one(s: pd.Series) -> pd.Series:\n","    return s + 1\n","\n","spark.udf.register(\"add_one\", add_one)\n","spark.sql(\"SELECT add_one(v1) FROM tableA\").show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+\n","|add_one(v1)|\n","+-----------+\n","|          2|\n","|          3|\n","|          4|\n","|          5|\n","|          6|\n","|          7|\n","|          8|\n","|          9|\n","+-----------+\n","\n"]}]},{"metadata":{"id":"cGMtw7fyDUuy"},"cell_type":"markdown","source":["These SQL expressions can directly be mixed and used as PySpark columns."]},{"metadata":{"trusted":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"iT5H0Mh6DUuy","executionInfo":{"status":"ok","timestamp":1647282348893,"user_tz":240,"elapsed":660,"user":{"displayName":"Yifeng Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiC6bmJK1d7ZKE43OYaNuYE_mKsuOW5o4Ms6o_X5g=s64","userId":"11510963676112107205"}},"outputId":"726e7a3d-5baf-42e8-eea3-6667371aa48e"},"cell_type":"code","source":["from pyspark.sql.functions import expr\n","\n","df.selectExpr('add_one(v1)').show()\n","df.select(expr('count(*)') > 0).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+\n","|add_one(v1)|\n","+-----------+\n","|          2|\n","|          3|\n","|          4|\n","|          5|\n","|          6|\n","|          7|\n","|          8|\n","|          9|\n","+-----------+\n","\n","+--------------+\n","|(count(1) > 0)|\n","+--------------+\n","|          true|\n","+--------------+\n","\n"]}]}]}