{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of DSE 503 (ECE 598) Mid-term exam 2122F (Your Name).ipynb","provenance":[{"file_id":"15d5PW5cKiI2k9qQRKwAOmUi5-xQ5Ci1C","timestamp":1648479955686}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["* Your Name:\n","* Deadline: 11am, March 26, Saturday\n","* This exam is open-book and open-note. However, you must complete it independently and no collaboration is allowed.\n","* Submission Instruction: Shared it with Yifeng.Zhu@maine.edu"],"metadata":{"id":"Ge8YHsxnznq8"}},{"cell_type":"markdown","source":["# Question 1 (10 points): AFR of hard drives"],"metadata":{"id":"54pXylkjzdmz"}},{"cell_type":"markdown","source":["You are to find the [Annualized Failure Rate](https://en.wikipedia.org/wiki/Annualized_failure_rate) of disks based on this [harddrive dataset](https://www.kaggle.com/datasets/backblaze/hard-drive-test-data).  \n","\n","This dataset consists of 3,179,295 records for a total of 65,993 hard drives. The records are collected during the period of 120 days, from 2016-01-01 to 2016-04-29. Note that each disk serial number can uniquely identify a disk globally.\n","\n","You need to find the up time for each disk during this 120-day period, and calculate the [Mean time between failures (MTBF)](https://en.wikipedia.org/wiki/Mean_time_between_failures):\n","\n","\\begin{equation}\n","\\text{MTBF} = \\frac{\\sum_{i}{(\\text{start time of disk i} - \\text{end time of disk i})}}{\\text{total number of disk failures}}\n","\\end{equation}\n","\n","Annualized failure rate (AFR) can be caculated using the following equation:\n","\n","\\begin{equation}\n","AFR = 1-e^{\\frac{-8766}{MTBF}}\n","\\end{equation}\n","\n","where Mean Time Between Failure (MTBF) are in hours, and there are 8766 hours in a year.\n"],"metadata":{"id":"WiFZVyKMTYZP"}},{"cell_type":"code","source":["# Download the dataset\n","!wget https://web.eece.maine.edu/~zhu/DL/archive.zip\n","# -o  overwrite files WITHOUT prompting\n","!unzip -o archive.zip\n","!ls"],"metadata":{"id":"D242kLLUEu8T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np"],"metadata":{"id":"idnw9SXNE2SP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read the hard drive dataset\n","df = pd.read_csv('./harddrive.csv')\n","print(df.shape)"],"metadata":{"id":"4dsLTcDVE6Ds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract relevant columns and ignore the other columns\n","hdd_df = df[['date', 'serial_number', 'failure']]\n","# sorting by serial number and then by date, to get the failure as last value (end of cycle)\n","hdd_df = hdd_df.sort_values(['serial_number', 'date'])\n","hdd_df.reset_index(inplace=True, drop=True)\n","hdd_df['date'] = pd.to_datetime(hdd_df['date'])\n","hdd_df.shape"],"metadata":{"id":"r2LtklTWQNNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hdd_df.head()"],"metadata":{"id":"iYNDjsGxWrRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# number of hdd\n","print(\"number of hdd:\", hdd_df['serial_number'].value_counts().shape) "],"metadata":{"id":"S07UORc4WF6D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Write code below to calculate the average MTBF of all drive"],"metadata":{"id":"RRBCqbTa0sZA"}},{"cell_type":"code","source":["# Put your code here\n","\n","\n"],"metadata":{"id":"saBGrpik0VWm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Write code below to calculate the anualized failure rate based on the average MTBF calculated above "],"metadata":{"id":"7otvoGIh0xh3"}},{"cell_type":"code","source":["# Put your code here\n","\n"],"metadata":{"id":"edOdMPio0jsi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 2 (10 points): Performance Evaluation"],"metadata":{"id":"YSrHSED1zhNP"}},{"cell_type":"markdown","source":["## Analysis of Strong Scaling via Amdahl's Law\n","We have introduced [Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law) in the class. Suppose the workload $W$ is fixed but the number of processors that can work on $W$ can be increased. The speedup of $N$ processor systems is defined as \n","\\begin{equation} \n","Speedup = \\frac{\\text{Time on 1 processor of solving W}}{\\text{Time on N processors of solving W}} = \\frac{T_s}{T_p} = \\frac{1}{(1-p) + \\frac{p}{N}}\n","\\end{equation}\n","where $T_s$ is the sequentail time running on a processor, $T_p$ is the parallel time running on N processors, and  $p$ is the time portion of the algorithm that can be parallelized.  \n","\n","Amdahl’s law states that, for a fixed problem, the upper limit of speedup is determined by the serial fraction of the code:\n","\\begin{equation}\n","Speedup < \\lim_{N \\to \\infty} \\frac{1}{(1-p) + \\frac{p}{N}} = \\frac{1}{1-p}\n","\\end{equation}\n","Amdal's law describes **strong scaling**, where the workload size is kept fixed."],"metadata":{"id":"BfyRR-j60HTM"}},{"cell_type":"markdown","source":["## Analysis of Weak Scaling via Gustafson's Law\n","The [Gustafson's Law](https://en.wikipedia.org/wiki/Gustafson%27s_law) uses a different assumption. While the Amdahl's law assumes that the workload size is fixed,  the Gustafson's law assumes that the parallel part increases as the system scales up but the serial part remains fixed.\n","\n","\\begin{eqnarray}\n","T_p & = & a + b \\\\\n","T_s & = & a + N \\times b \\\\\n","\\end{eqnarray}\n","\n","where $a$ and $b$ is time spent executing the serial parts and the parallel parts on the parallel systems, respectively.\n","\n","Therefore, the speedup (often called ***scaled speedup***) is\n","\n","\\begin{eqnarray}\n","Speedup & = & \\frac{T_s}{T_p} \\\\\n","  & = &  \\frac{a + N \\times b}{a + b} \\\\\n","  & = &  s + (1-s) \\times N \\\\\n","\\end{eqnarray}\n","\n","\n","where $s$ is the fractions of time spent executing the serial parts, and $s = \\frac{a}{a+b}$. \n","\n","Alternatively, the speedup can be expressed using $p$, which is the fraction of time spent executing the parallel parts on the parallel systems, and $s + p = 1$\n","\\begin{equation}\n","Speedup = (1-p) + p \\times N = 1 + p \\times (N -1)\n","\\end{equation}\n","\n","Gustafson's law describes **weak scaling**, where the scaled speedup is calculated based on the amount of work done for a scaled problem size (in contrast to Amdahl’s law which focuses on fixed problem size). "],"metadata":{"id":"7qYm3OPFRitG"}},{"cell_type":"code","source":["#@title\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def amdahls_law(p, N):\n","    return ((1-p) + p/N)**-1\n","\n","def gustafson_law(p, N):\n","    return (1-p) + p*N\n","\n","N = np.arange(1, 100)\n","\n","plt.subplots(nrows=1, ncols=2, figsize=(16, 7))\n","\n","plt.subplot(1, 2, 1)\n","plt.title(\"Amdahl's Law\")\n","plt.plot(N, [amdahls_law(0.5, n) for n in N], label=\"p = 50%\")\n","plt.plot(N, [amdahls_law(0.75, n) for n in N], label=\"p = 75%\")\n","plt.plot(N, [amdahls_law(0.90, n) for n in N], label=\"p = 90%\")\n","plt.plot(N, [amdahls_law(0.95, n) for n in N], label=\"p = 95%\")\n","\n","plt.legend()\n","plt.xlabel(\"Number of processes\")\n","plt.ylabel(\"Speedup\")\n","plt.grid()\n","\n","plt.subplot(1, 2, 2)\n","plt.title(\"Gustafson's Law\")\n","plt.plot(N, [gustafson_law(0.5, n) for n in N], label=\"p = 50%\")\n","plt.plot(N, [gustafson_law(0.75, n) for n in N], label=\"p = 75%\")\n","plt.plot(N, [gustafson_law(0.90, n) for n in N], label=\"p = 90%\")\n","plt.plot(N, [gustafson_law(0.95, n) for n in N], label=\"p = 95%\")\n","plt.legend()\n","plt.xlabel(\"Number of processes\")\n","plt.ylabel(\"Speedup\")\n","plt.grid()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"cellView":"form","id":"ReuNOUW1Ofl0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question: \n","\n","Supposed you are to investigate the scalability of parallel software programs in which the serial portion would not grow as the program size increase. For example, a program must read file data serailly, but the time to read these data is indepdendent of the problem size. Should strong scaling anlysis or weak scaling analysis be prefered? Why?"],"metadata":{"id":"SPEhSfzvRFn8"}},{"cell_type":"markdown","source":["**Answer**:\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"rbEEbIMPHokN"}},{"cell_type":"markdown","source":["# Question 3 (10 points): Comparing Systems"],"metadata":{"id":"Q_H4Cz5Nziot"}},{"cell_type":"markdown","source":["We are comparing two data compression algorithms: *bzip* and *gzip*. The following data consists of three columns: \n","1.   orginal file size\n","2.   file size after compression by bzip\n","3.   file size after compression by gzip\n","\n","At the 90% confidence level, can we say bzip compresses better than gzip?"],"metadata":{"id":"wOCfJh530Hy8"}},{"cell_type":"code","source":["!wget https://web.eece.maine.edu/~zhu/DL/compress.csv"],"metadata":{"id":"NbBMibg5PubF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import math\n","import scipy.stats as stats\n","import matplotlib.pyplot as plt\n","import statistics"],"metadata":{"id":"d8vnQ7oHQdy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('./compress.csv')\n","df.shape"],"metadata":{"id":"c1r2DjcdO86n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"PimgfdGcUBLE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write your code below\n","\n","\n","\n","\n"],"metadata":{"id":"UsjDzB2G1QCd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 4 (10 points): Parallel Computing"],"metadata":{"id":"iOHzgPwdzlZT"}},{"cell_type":"markdown","source":["The following code calculates the sum of array using **Send()** and **Recv()**. Rewrite the code using **Scatter()** and **Reduce()** to improve the efficiency of the code. You might find these two demos are helpful [MPI demo 1](https://colab.research.google.com/drive/18VG_B11qoDMm2wzfNXl8MayxyHy6x6om?usp=sharing), and [MPI demo 2](https://colab.research.google.com/drive/1b21L9_yZG7sbCN66B-6k5ZV0EkyH4pB2?usp=sharing)."],"metadata":{"id":"oscqRFC40IvY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP8c9PRoyipZ"},"outputs":[],"source":["!pip install mpi4py"]},{"cell_type":"code","source":["%%file sum.py\n","from mpi4py import MPI\n","import numpy as np\n","\n","comm = MPI.COMM_WORLD\n","rank = comm.Get_rank()\n","size = comm.Get_size()\n","N = 10\n","\n","if(rank == 0):\n","    data = np.arange(N*size, dtype='i')\n","    for i in range(1, size):\n","        SLICE = data[i*N:(i+1)*N]\n","        comm.Send([SLICE, MPI.INT], dest=i)\n","    myData = data[0:N]\n","else:\n","    myData = np.empty(N, dtype='i')\n","    comm.Recv([myData, MPI.INT], source=0)\n","\n","S = sum(myData)\n","print(rank, 'has data', myData, 'sum =', S)\n","\n","sums = np.zeros(size, dtype='i')\n","if(rank > 0):\n","    comm.send(S, dest=0)\n","else:\n","    sums[0] = S\n","    for i in range(1, size):\n","        sums[i] = comm.recv(source=i)\n","    print('total sum =', sum(sums))"],"metadata":{"id":"m2_5ZBSaJIvM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mpirun --allow-run-as-root -n 4 python sum.py"],"metadata":{"id":"QN8ydUZeJNcd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Rewrite the above MPI program using `Scatter()` (or scatter()) and `Reduce()` (or reduce()), instead of `Send()`/`send()` and `Recv()`/`recv()`."],"metadata":{"id":"8JHYuHGF1YbV"}},{"cell_type":"code","source":["%%file sum_new.py\n","from mpi4py import MPI\n","import numpy as np\n","\n","comm = MPI.COMM_WORLD\n","rank = comm.Get_rank()\n","size = comm.Get_size()\n","N = 10\n","\n","# put your new code that use scatter() and reduce() below\n","\n","\n","\n"],"metadata":{"id":"5-yWUH9MfI73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mpirun --allow-run-as-root -n 4 python sum_new.py"],"metadata":{"id":"PaPO3e8ffJAC"},"execution_count":null,"outputs":[]}]}