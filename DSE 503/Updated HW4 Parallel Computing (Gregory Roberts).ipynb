{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Updated HW4 Parallel Computing (Gregory Roberts).ipynb","provenance":[{"file_id":"1jjK8d9e86ZiKWqd5OHuTJ80BG-zXX0-2","timestamp":1649084925792},{"file_id":"1Z0IW7b6xGMDJ7bqhqnaAQnCLjcdJkxJO","timestamp":1647134957182}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Your Name:  Gregory Roberts\n","\n","Submission Instruction:\n","* Instruction: make a copy of this CoLab file and share it with me (Yifeng.Zhu@maine.edu).\n","* Deadline: Midnight, Sunday, April 2\n"],"metadata":{"id":"6l93mpX3DmeN"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bClHRKnEcB3E","executionInfo":{"status":"ok","timestamp":1651101654937,"user_tz":300,"elapsed":3479,"user":{"displayName":"Gregory Roberts","userId":"00352358460499938030"}},"outputId":"64e0271b-471c-4dc3-e4b9-5e72ffb573f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mpi4py in /usr/local/lib/python3.7/dist-packages (3.1.3)\n"]}],"source":["!pip install mpi4py"]},{"cell_type":"markdown","source":["#Question 1: Point-to-point communication"],"metadata":{"id":"hk14eH9XBCn7"}},{"cell_type":"markdown","source":["Is there any problem in the following code? If so, how to fix it?"],"metadata":{"id":"I7sWxNKhDTdV"}},{"cell_type":"code","source":["%%file q1.py\n","from mpi4py import MPI\n","\n","comm = MPI.COMM_WORLD\n","rank = comm.Get_rank()\n","\n","if rank==1:\n","    data_send= \"a\"\n","    destination_process = 2\n","    source_process = 2\n","\n","    comm.send(data_send, dest=destination_process)\n","    data_received = comm.recv(source=source_process)\n","    \n","    print (\"sending data %s \" %data_send + \\\n","           \"to process %d\" %destination_process)\n","    print (\"data received = %s\" %data_received)\n","\n","if rank==2:\n","    data_send= \"b\"\n","    destination_process = 1\n","    source_process = 1\n","\n","    comm.send(data_send, dest=destination_process)\n","    data_received = comm.recv(source=source_process)\n","    \n","    print (\"sending data %s :\" %data_send + \\\n","           \"to process %d\" %destination_process)\n","    print (\"data received = %s\" %data_received)\n"],"metadata":{"id":"h6r-IMmuC7_f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648923779409,"user_tz":300,"elapsed":134,"user":{"displayName":"Gregory Roberts","userId":"00352358460499938030"}},"outputId":"71c9ad9e-9bd3-482e-c695-032e391eee13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing q1.py\n"]}]},{"cell_type":"code","source":["!mpirun --allow-run-as-root -n 4 python q1.py"],"metadata":{"id":"9h3HAro7DJZ9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648923782663,"user_tz":300,"elapsed":604,"user":{"displayName":"Gregory Roberts","userId":"00352358460499938030"}},"outputId":"d1e382d3-8aab-4cf1-ac32-d3085e6d0799"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sending data a to process 2\n","data received = b\n","sending data b :to process 1\n","data received = a\n"]}]},{"cell_type":"markdown","source":["**Your answer:**  Changed the code so that the comm.send is before the comm.recv for both rank processes.\n"],"metadata":{"id":"SHvRjOIbDaB6"}},{"cell_type":"markdown","source":["# Question 2: Fix the bug of the following code"],"metadata":{"id":"ox8qkWQQcFIQ"}},{"cell_type":"markdown","source":["The following code is to implement a parallel matrix vector product. However, there are a large different between the parallel implementation and the result produced by numpy.dot. What is wrong with the code and how to fix it?"],"metadata":{"id":"19DJj189Bf2K"}},{"cell_type":"code","source":["%%matvec.py%%\n","import numpy as np\n","from mpi4py import MPI\n","import os\n","\n","# Parallel matrix-vector product\n","def matvec(comm, A, x):\n","    size = comm.Get_size()\n","    print('size ', size)\n","    rank = comm.Get_rank()\n","    m = A.shape[0] // size # local rows\n","    # every process gets a part of the data\n","    y_part = np.dot(A[rank * m:(rank+1)*m, :], x) \n","    # container for the result    \n","    y = np.zeros_like(x, dtype='double')     \n","    # collect results from the pool, write them to container y      \n","    comm.Allgather([y_part,  MPI.DOUBLE], [y, MPI.DOUBLE])    \n","    return y\n","\n","n = 400\n","comm = MPI.COMM_WORLD\n","\n","\n","rank = comm.Get_rank()\n","\n","\n","# GR - initialize vector and matrix to empty values\n","x = np.empty(n, dtype='d')\n","A = np.empty([n,n], dtype='d')\n","\n","# GR - generate random vector and matrix\n","if rank == 0:\n","  x = np.random.rand(n)     # Generate a vector     \n","  A = np.random.rand(n, n)  # Generate a nxn matrix\n","\n","# GR - broadcast the vector and matrix defined in rank 0\n","comm.Bcast([x, MPI.DOUBLE], root=0)\n","comm.Bcast([A, MPI.DOUBLE], root=0)\n","\n","y_mpi = matvec(comm, A, x) # y_mpi = A * x\n","\n","if rank == 0: # check \n","  y = np.dot(A, x)      \n","\n","  # compare the local and MPI results   \n","  # The output should be a very small value \n","  print(\"sum(y - y_mpi) = \", (y - y_mpi).sum()) \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCJ1U_R8cTzU","executionInfo":{"status":"ok","timestamp":1651089335634,"user_tz":300,"elapsed":137,"user":{"displayName":"Gregory Roberts","userId":"00352358460499938030"}},"outputId":"f6d6602b-c604-4bb8-9dfe-761615576b46"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","size  1\n","sum(y - y_mpi) =  0.0\n"]}]},{"cell_type":"code","source":["!mpirun --allow-run-as-root -n 4 python3 matvec.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HflMbWWcaTD","executionInfo":{"status":"ok","timestamp":1649086389047,"user_tz":300,"elapsed":1795,"user":{"displayName":"Gregory Roberts","userId":"00352358460499938030"}},"outputId":"ebe185e1-4f2b-43a5-faa2-880e01117d0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sum(y - y_mpi) =  8.526512829121202e-14\n"]}]},{"cell_type":"markdown","source":["**Your answer:**  I tried a number of different processes. I tried to use numpy.matmul, @, *, etc. I tried to split the numpy.dot into separate processes, similar to what was being done in matvec.\n","\n","I found that for each rank that the A and x arrays were loaded with new numbers for the matvec function. The y dot production would only run once when the rank is 0 (zero).\n","\n","By using static values stored in files, and then read in. The values stay the same each time, and y_mpi is not loaded with new random values each time the process iterates through the rank. Which produces a smaller value between y and y_mpi.\n","\n"],"metadata":{"id":"YnJERNsaDj9y"}}]}